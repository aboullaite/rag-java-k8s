package me.aboullaite.rag.orchestrator.service;

import me.aboullaite.rag.common.dto.CitationInfo;
import me.aboullaite.rag.common.dto.GenerationResponse;
import me.aboullaite.rag.common.dto.Query;
import me.aboullaite.rag.common.dto.RetrievedDoc;
import me.aboullaite.rag.common.tracing.TracingUtils;
import me.aboullaite.rag.orchestrator.cache.SemanticCacheService;
import me.aboullaite.rag.orchestrator.cache.SemanticCacheService.CacheEntry;
import me.aboullaite.rag.orchestrator.cache.SemanticCacheService.CacheHit;
import me.aboullaite.rag.orchestrator.client.LlmClient;
import me.aboullaite.rag.orchestrator.client.LlmClient.LlmResponse;
import me.aboullaite.rag.orchestrator.client.RetrieverClient;
import me.aboullaite.rag.orchestrator.config.OrchestratorProperties;
import me.aboullaite.rag.orchestrator.embedding.EmbeddingService;
import me.aboullaite.rag.orchestrator.prompt.PromptAssembler;
import me.aboullaite.rag.orchestrator.prompt.PromptAssembler.PromptBundle;
import io.micrometer.core.instrument.Counter;
import io.micrometer.core.instrument.DistributionSummary;
import io.micrometer.core.instrument.MeterRegistry;
import io.micrometer.core.instrument.Timer;
import io.opentelemetry.api.GlobalOpenTelemetry;
import io.opentelemetry.api.trace.Span;
import io.opentelemetry.api.trace.Tracer;
import java.util.List;
import java.util.Map;
import java.util.regex.Pattern;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.stereotype.Service;
import org.springframework.util.StringUtils;
import reactor.core.publisher.Mono;

@Service
public class AskService {

    private static final Logger log = LoggerFactory.getLogger(AskService.class);
    private static final Pattern PII_PATTERN = Pattern.compile("(\\d{3}-\\d{2}-\\d{4})|(\\d{16})|([A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+)");

    private final RetrieverClient retrieverClient;
    private final LlmClient llmClient;
    private final SemanticCacheService cacheService;
    private final EmbeddingService embeddingService;
    private final PromptAssembler promptAssembler;
    private final OrchestratorProperties properties;
    private final Timer askLatency;
    private final Counter cacheHitCounter;
    private final Counter cacheMissCounter;
    private final Counter fallbackCounter;
    private final Counter tokensCounter;
    private final DistributionSummary costSummary;
    private final Tracer tracer;
    private final MeterRegistry meterRegistry;

    public AskService(
            RetrieverClient retrieverClient,
            LlmClient llmClient,
            SemanticCacheService cacheService,
            EmbeddingService embeddingService,
            PromptAssembler promptAssembler,
            OrchestratorProperties properties,
            MeterRegistry meterRegistry) {
        this.retrieverClient = retrieverClient;
        this.llmClient = llmClient;
        this.cacheService = cacheService;
        this.embeddingService = embeddingService;
        this.promptAssembler = promptAssembler;
        this.properties = properties;
        this.meterRegistry = meterRegistry;
        this.askLatency = Timer.builder("rag_orchestrator_latency")
                .description("End-to-end /v1/ask latency")
                .register(meterRegistry);
        this.cacheHitCounter = Counter.builder("rag_cache_hit_total")
                .description("Semantic cache hits")
                .register(meterRegistry);
        this.cacheMissCounter = Counter.builder("rag_cache_miss_total")
                .description("Semantic cache misses")
                .register(meterRegistry);
        this.fallbackCounter = Counter.builder("rag_generator_fallback_total")
                .description("Number of times deterministic fallback answered")
                .register(meterRegistry);
        this.tokensCounter = Counter.builder("rag_tokens_generated_total")
                .description("Total tokens generated by model responses")
                .register(meterRegistry);
        this.costSummary = DistributionSummary.builder("rag_cost_usd_total")
                .description("Approximate request cost in USD")
                .register(meterRegistry);
        this.tracer = GlobalOpenTelemetry.getTracer("rag-java/orchestrator");
    }

    public Mono<GenerationResponse> ask(String prompt, Map<String, String> filters, Integer topK) {
        String sanitizedPrompt = redact(prompt);
        double[] embedding = embeddingService.embed(sanitizedPrompt);
        Span span = tracer.spanBuilder("rag.ask")
                .setAttribute("rag.prompt.length", sanitizedPrompt.length())
                .startSpan();
        Timer.Sample sample = Timer.start(meterRegistry);

        return cacheService.lookup(sanitizedPrompt, embedding)
                .flatMap(hit -> onCacheHit(hit, span))
                .switchIfEmpty(Mono.defer(() -> {
                    cacheMissCounter.increment();
                    TracingUtils.recordCacheHit(span, false);
                    return generateWithRetrieval(sanitizedPrompt, filters, topK, embedding, span);
                }))
                .doOnError(span::recordException)
                .doFinally(signalType -> {
                    sample.stop(askLatency);
                    span.end();
                });
    }

    private Mono<GenerationResponse> onCacheHit(CacheHit hit, Span span) {
        cacheHitCounter.increment();
        TracingUtils.recordCacheHit(span, true);
        CacheEntry entry = hit.entry();
        GenerationResponse.ResponseMetadata metadata = new GenerationResponse.ResponseMetadata(
                true,
                hit.similarity(),
                null,
                false
        );
        GenerationResponse response = new GenerationResponse(entry.answer(), entry.citations(), List.of(), false, metadata);
        TracingUtils.recordModelUsage(span, properties.getModelName(), 0, response.answer().split("\\s+").length);
        return Mono.just(response);
    }

    private Mono<GenerationResponse> generateWithRetrieval(
            String sanitizedPrompt,
            Map<String, String> filters,
            Integer topK,
            double[] embedding,
            Span parentSpan) {
        Query query = new Query(sanitizedPrompt, filters, topK == null ? 0 : topK);
        return retrieverClient.retrieve(query)
                .flatMap(docs -> produceAnswer(sanitizedPrompt, docs, embedding, parentSpan))
                .switchIfEmpty(Mono.defer(() -> produceAnswer(sanitizedPrompt, List.of(), embedding, parentSpan)));
    }

    private Mono<GenerationResponse> produceAnswer(
            String sanitizedPrompt,
            List<RetrievedDoc> docs,
            double[] embedding,
            Span parentSpan) {
        PromptBundle promptBundle = promptAssembler.assemble(sanitizedPrompt, docs);
        return llmClient.generate(promptBundle.prompt())
                .map(response -> toGenerationResponse(response, promptBundle, false, parentSpan))
                .flatMap(response -> cacheService.put(sanitizedPrompt, embedding, response, docs)
                        .thenReturn(response))
                .onErrorResume(ex -> {
                    log.warn("LLM call failed, using fallback: {}", ex.getMessage());
                    fallbackCounter.increment();
                    TracingUtils.recordFallback(parentSpan, ex.getClass().getSimpleName());
                    GenerationResponse fallback = fallbackResponse(docs, promptBundle.citationDetails());
                    return cacheService.put(sanitizedPrompt, embedding, fallback, docs)
                            .onErrorResume(e -> Mono.empty())
                            .thenReturn(fallback);
                });
    }

    private GenerationResponse toGenerationResponse(
            LlmResponse llmResponse,
            PromptBundle bundle,
            boolean partial,
            Span span) {
        int tokens = llmResponse.tokens();
        tokensCounter.increment(tokens);
        TracingUtils.recordModelUsage(span, properties.getModelName(), llmResponse.ttftMillis(), tokens);
        double cost = tokens * 0.000002d;
        costSummary.record(cost);
        String answerWithCitations = ensureCitations(llmResponse.answer(), bundle.citations());
        GenerationResponse.ResponseMetadata metadata = new GenerationResponse.ResponseMetadata(
                false,
                null,
                "vector",
                false
        );
        return new GenerationResponse(answerWithCitations, bundle.citations(), bundle.citationDetails(), partial, metadata);
    }

    private GenerationResponse fallbackResponse(List<RetrievedDoc> docs, List<CitationInfo> citationDetails) {
        GenerationResponse.ResponseMetadata metadata = new GenerationResponse.ResponseMetadata(
                false,
                null,
                docs.isEmpty() ? null : "vector",
                true
        );
        if (docs.isEmpty()) {
            return new GenerationResponse("I don't know.", List.of(), List.of(), true, metadata);
        }
        StringBuilder builder = new StringBuilder("Here's what I found:\n");
        for (RetrievedDoc doc : docs.stream().limit(3).toList()) {
            builder.append("- [")
                    .append(doc.id())
                    .append("] ")
                    .append(truncate(doc.chunk(), 220))
                    .append("\n");
        }
        List<String> citations = docs.stream().limit(3).map(RetrievedDoc::id).toList();
        List<CitationInfo> limitedDetails = citationDetails.stream().limit(3).toList();
        return new GenerationResponse(builder.toString(), citations, limitedDetails, true, metadata);
    }

    private String ensureCitations(String answer, List<String> citations) {
        if (citations.isEmpty()) {
            return answer;
        }
        boolean alreadyCited = citations.stream().anyMatch(answer::contains);
        if (alreadyCited) {
            return answer;
        }
        return answer + "\n\nSources: " + citations.stream()
                .map(id -> "[" + id + "]")
                .toList();
    }

    private String redact(String prompt) {
        if (!StringUtils.hasText(prompt)) {
            return "";
        }
        return PII_PATTERN.matcher(prompt).replaceAll("[redacted]");
    }

    private String truncate(String value, int max) {
        if (value == null) {
            return "";
        }
        return value.length() <= max ? value : value.substring(0, max) + "...";
    }
}
